\documentclass[aoas,preprint, 11pt, dvipsnames, table, x11name]{imsart}

% natbib citation styles, see the natbib documentation, a copy of which
\usepackage{tikz-cd}
\usepackage[toc,page]{appendix}
\usepackage[font={small}, labelfont=bf]{caption}
\usepackage{subcaption}
\usepackage[utf8]{inputenc}
\newcommand{\E}{\mbox{E}}
\newcommand{\N}{\mbox{N}}
\usepackage{amsfonts}
%Font
\usepackage[T1]{fontenc}
\usepackage{comment}
%\usepackage[dvipsnames,table,x11names]{xcolor}
\usepackage{xcolor}
\definecolor{darkgreen}{RGB}{0,69,0} %for rf
\definecolor{navy}{RGB}{0,60,113} %tough choice
\usepackage{booktabs}
\usepackage{bbm}
\usepackage{array}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}

\setlength{\parskip}{\baselineskip}
\usepackage{imakeidx}
\usepackage{tikz}
\usetikzlibrary{arrows, arrows.meta, calc, positioning}
%\usetikzlibrary{arrows}
\usetikzlibrary{positioning}
\newdimen\nodeDist{}
\nodeDist=25mm
\usepackage{pgfplots}
\newcommand{\indep}{\rotatebox[origin=c]{90}{$\models$}}
%\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{amsmath,commath,amssymb,blkarray,bm,bbm}
\usepackage{bbm}
\usepackage{mathtools}
\newcommand{\imp}[1]{\textbf{#1}}
\renewcommand{\bm}[1]{\mathbf{#1}}
\usepackage{mathrsfs}
\usepackage{physics}
\usepackage[pagebackref]{hyperref}  
\usepackage{comment}
\usepackage[authoryear]{natbib}
\bibliographystyle{plainnat}
\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\DeclareMathOperator{\NA}{NA}
\newcommand{\ind}[1]{\mathbbm{1}({#1})}%
\hypersetup{
	colorlinks=true,
	linkcolor={blue!60!black},
	filecolor=magenta,      
	urlcolor={blue!60!black},
	citecolor={blue!60!black}
}
\bibliographystyle{imsart-nameyear}
\urlstyle{same}
%% Packages
\RequirePackage{amsthm,amsmath,amsfonts,amssymb}
\RequirePackage[authoryear]{natbib}
%\RequirePackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}
%\RequirePackage{graphicx}

\startlocaldefs
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Uncomment next line to change            %%
%% the type of equation numbering           %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\numberwithin{equation}{section}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% For Axiom, Claim, Corollary, Hypothezis, %%
%% Lemma, Theorem, Proposition              %%
%% use \theoremstyle{plain}                 %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\theoremstyle{plain}
\newtheorem{axiom}{Axiom}
\newtheorem{claim}[axiom]{Claim}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% For Assumption, Definition, Example,     %%
%% Notation, Property, Remark, Fact         %%
%% use \theoremstyle{remark}                %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{remark}
\newtheorem{definition}[theorem]{Definition}
\newtheorem*{example}{Example}
\newtheorem*{fact}{Fact}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Please put your definitions here:        %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\endlocaldefs


\begin{document}

\begin{frontmatter}
\title{Do forecasts of bankruptcy cause bankruptcy? \\A machine learning sensitivity analysis.}
%\title{A sample article title with some additional note\thanksref{t1}}
\runtitle{Do Forecasts of Bankruptcies cause Bankruptcies?}
%\thankstext{T1}{A sample additional note to the title.}

\begin{aug}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%Only one address is permitted per author. %%
%%Only division, organization and e-mail is %%
%%included in the address.                  %%
%%Additional information can be included in %%
%%the Acknowledgments section if necessary. %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\author[A]{\fnms{Demetrios} \snm{Papakostas}\ead[label=e2,mark]{dpapakos@asu.edu}}
\author[A]{\fnms{P. Richard} \snm{Hahn}\ead[label=e1,mark]{prhahn@asu.edu}},
\author[B]{\fnms{Jared} \snm{Murray}\ead[label=e3,mark]{jared.murray@mccombs.utexas.edu}}
\and
\author[C]{\fnms{Frank} \snm{Zhou}\ead[label=e4,mark]{szho@wharton.upenn.edu}}
\author[D]{\fnms{Joseph} \snm{Gerakos}\ead[label=e5,mark]{Joseph.J.Gerakos@tuck.dartmouth.edu}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Addresses                                %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\address[A]{School of Mathematical and Statistical Sciences,
Arizona State University,
\printead{e1,e2}}


\address[B]{Department of Information, Risk and Operations Management,
The University of Texas at Austin,
\printead{e3}}



\address[C]{The Wharton School,
University of Pennsylvania,
\printead{e4}}

\address[D]{Tuck School of Business,
Dartmouth College,
\printead{e5}}
\end{aug}

\begin{abstract}
 It is widely speculated that auditors' public forecasts of bankruptcy are, at least in part, self-fulfilling prophecies in the sense that they might actually cause bankruptcies that would not have otherwise occurred. This conjecture is hard to prove, however, because the strong association between bankruptcies and bankruptcy forecasts could simply indicate that auditors are skillful forecasters with unique access to highly predictive covariates. 
 %Although this question could be addressed with an instrumental variables analysis, the availability of valid instruments is hotly debated.
In this paper, we investigate the causal effect of bankruptcy forecasts on bankruptcy using nonparametric sensitivity analysis. We contrast our analysis with two alternative approaches:  a linear bivariate probit model with an endogenous regressor, and a recently developed bound on risk ratios called E-values. Additionally, our machine learning approach incorporates a monotonicity constraint corresponding to the assumption that bankruptcy forecasts do not make bankruptcies less likely. Finally, a tree-based posterior summary of the treatment effect estimates allows us to explore which observable firm characteristics moderate the inducement effect.
\end{abstract}

\begin{keyword}
\kwd{BART}
\kwd{Causal Inference}
\kwd{heterogeneous treatment effects}
\kwd{self-fulfilling prophecy}
\kwd{sensitivity analysis}
\end{keyword}

\end{frontmatter}
\section{Introduction}
A ``going concern opinion'' is an assessment by an auditor that a firm is at risk of going out of business in the coming year. Here a ``concern'' refers to a firm, and ``going'' refers to staying, as opposed to going out of, business. By U.S. accounting law, a public company receiving a going concern opinion must disclose it in their annual filings.  See \cite{maurer-wsj-2020} for a recent discussion of going concern opinions in the news. Once issued, a going concern opinion may directly contribute to a firm's bankruptcy risk, for example, by inducing lenders to pull lines of credit. As reported in \cite{maurer-wsj-2020}:

\begin{quote}
Companies that receive a going-concern audit opinion may be subjected to more rigorous covenant terms or downgrades in their credit ratings, said Anna Pinedo, a partner at law firm Mayer Brown. Fractured relationships with customers could also strengthen a businessâ€™s competitors, she said.
\end{quote}

Estimating the magnitude of such an ``inducement effect'' is complicated by the unavailability of the auditors' private information to the analyst. That is, in addition to publicly available firm information, auditors have access to ``private information'' gleaned from confidential documents and via firsthand knowledge of undocumented attributes such as the firm's corporate culture. This paper considers the question: do going-concern opinions help to predict bankruptcy because they incorporate the auditor's private information or because of an inducement effect? This is a textbook example of causal inference where the potential unobserved confounders are particularly pictureseque: what do auditors know that we (the analysts) do not? In this paper, we introduce methodology to quantify the impact of private information on the probability that a firm files for bankruptcy in the fiscal year following the issuance of a going concern opinion.  We conduct a sensitivity analysis rooted in nonlinear, semiparametric regression techniques and a generalization of the bivariate probit model with an endogenous regressor. We conclude that there is evidence for inducement under plausible assumptions on the distribution of the auditors' private information.



\subsection{Methodological background}

Denote the treatment variable by $G_i$ for ``going concern'' so that $G_i = 1$ for the $i$th firm in our sample if that firm disclosed a going concern opinion in the prior year. 

Denote the outcome variable $B_i$ for ``bankrupt'' so that $B_i = 1$ filed for bankruptcy, 
  In terms of potential outcomes \citep{rubin}, we are interested in two scenarios: $B^1_i$ and $B^0_i$, which are respectively the outcome of a firm $i$ if it had received the treatment and if it had not received the treatment, respectively; only one of these potential outcomes is observed. 


The primary estimand of interest will be the causal risk ratio (CRR):
\begin{equation}
	\tau \equiv \E(B^1)/\E(B^0)
\end{equation}
which we will often refer to as simply the ``inducement effect.'' Alternatively, the inducement effect in terms of the ``do''-operator of \cite{pearl-2000} as 
 \begin{equation}
	\tau \equiv \E(B=1 \mid \text{do}(G=1))/\E(B=1 \mid \text{do}(G=0))
\end{equation}
where $\text{do}(G=g)$ refers to an exogeneous intervention, in contradistinction to probabilistic conditioning. We will also consider the risk difference
\begin{equation}
	\Delta \equiv \E(B^1) - \E(B^0)
\end{equation}
and consider how these two estimands differ as a function of observable firm characteristics.

The fundamental problem of causal inference \citep{Holland-1986} is that $(B^1, B^0)$ are never observed simultaneously, rather only one or the other is observed. Consequently, the conditions under which CRR can be estimated must be carefully assessed and their plausibility debated. There are three widely used methods for estimating average treatment effects: randomization, regression adjustment (broadly construed to include matching and propensity score based methods), and instrumental variables analysis. To briefly review:
\begin{itemize}
\item In a randomized controlled trial the treatment variable --- $G$ in the present context --- is independent of the potential outcomes $B^1$ and $B^0$; in this case $\E(B^1) = \E(B \mid G = 1)$, the right hand side of which is readily estimable from observed data (and likewise for the $G = 0$ case). 

\item When a randomized experiment is not possible (such as in the present example) one instead may hope to find a set of control variables $\mathbf{x}$ for which $\E(B^1 \mid \mathbf{x}) = \E(B \mid G = 1, \mathbf{x})$ and $\E(B^0 \mid \mathbf{x}) = \E(B \mid G = 0, \mathbf{x})$, in which case treatment effects can be estimated by estimating these conditional expectations via regression modeling. This condition in called {\em conditional ignorability} or, alternatively, $\mathbf{x}$ are said to satisfy the {\em back-door criterion}. 

\item A third possibility is that a sufficient set of controls is unavailable, but an {\em instrument} for the treatment assignment is available. An instrument is a variable that is causally related to the treatment but not otherwise associated with the response variable. In the current context, an instrument variable (IV) would be a one that affects the probability that an auditor issues a going concern opinion without directly affecting bankruptcy probabilities or sharing common causes with bankruptcies. 
Here we do not elaborate on the details of instrumental variable regression, but see \cite{imbens2014instrumental} for a recent survey and \cite{larcker2010use} for a discussion of the use of IV specifically in accounting research.
\end{itemize}
%\cite{Imbens-rdd}, \cite{Thistlethwaite-rdd}, \cite{card1994}, 
%\cite{abadie2010}, \cite{abadie2003}
In the present context, none of these three approaches are available. A sufficient set of controls is certainly not readily available and the existence of a valid instrument is doubtful because firms choose their own auditing agency, rendering auditor attributes endogenous. Although there are other approaches --- such as regression continuity design, see \cite{Imbens-rdd} and \cite{Thistlethwaite-rdd}, difference-in-differences, see  \cite{card1994}, and the synthetic control method see \cite{abadie2010} and \cite{abadie2003} --- they apply in idiosyncratic settings that do not apply to the bankruptcy inducement problem. 

With none of the usual tools available to us, it may be possible to make additional modeling assumptions that yield identification of the treatment effect. One such model for bivariate binary observations is the bivariate probit model with an endogeneous regressor \citep[Section~ 15.7.3]{Wooldridge-2010}. Such model-based identification is generally undesirable because the identifying form of the likelihood typically lacks plausible justification \citep{Manski}. Accordingly, it is prudent to consider a range of different assumptions (model specifications) and observe how the estimated treatment effects vary as a result. In this paper, we propose a method for modeling the strength of unobserved confounding in a machine learning framework which permits convenient sensitivity analysis without constraining the observed data distribution unrealistically.

\subsection{Methodological contribution of this paper}
This paper brings together three lines of methodological research. First, we develop a generalization of the bivariate probit with endogeneous regressor and use this unidentified model to conduct a sensitivity analysis. Second, we utilize modern Bayesian tree-based classification models to estimate the identified parameters in our model and describe a numerical procedure to map these parameters back to the causal estimands of interest. This approach represents both a novel use of Bayesian machine learning as well as a novel application of machine learning to the applied problem of whether going concern opinions induce bankruptcy. Additionally, this model incorporates the assumption that going concern opinions cannot make bankruptcies less likely, a plausible assumption that potentially improves estimation accuracy. Finally, we apply a tree-based posterior summarization strategy to our estimates of the individual treatment effects to identify interesting subgroups for further scrutiny, a method first described in \cite{bcf}, building on a framework laid out in \cite{Hahn-2015} for linear models.

\subsection{Paper structure}
Because this work touches on many disparate areas, an overview organizing the contents may be helpful.
\begin{itemize}
\item First, we review the traditional parametric model used for the binary-treatment-binary-response setting with unmeasured confounding, which is the bivariate probit model with endogenous regressor. We provide a novel justification of this model in terms of Pearl's causal calculus using a latent factor representation of the bivariate probit likelihood. 
\item Next, we generalize this model by relaxing the linearity and distributional assumptions, making it robust to misspecification. The two approaches are compared via a simulation study.
\item The generalized bivariate probit model is not point identified, making a sensitivity analysis necessary. A computationally efficient method for conducting the sensitivity analysis is developed, which uses a single Bayesian model fit of the reduced form parameters.
\item We then introduce monotone Bayesian additive regression trees, which is a custom modification of the popular BART model \citep{bart}, and describe the Markov chain updates for enforcing monotonicity in the treatment variable.
\item Putting these pieces together, the new machine learning sensitivity analysis is applied to over 25,000 data points from publicly traded U.S. firms. Decision trees are used as a posterior summarization tool to discover variables that moderate the risk ratio.  
\item Additionally, the new approach is investigated via several simulation studies in which it is compared to the bivariate probit model and a recent bound-based approached called E-values \citep{Peng-2016} (which generalize the well known Cornfield bounds \citep{Cornfield}).
\end{itemize}

\section{The bivariate probit model with endogenous predictor}

A well-known model that has been used for problems similar to the one described here is the bivariate probit with endogenous predictor \citep[Section~ 15.7.3]{Wooldridge-2010}. This model can be expressed in terms of bivariate Gaussian latent utilities $Z_g$ and $Z_b$ that related to going concern opinions and bankruptcy, respectively: 
 \begin{equation}
 \begin{pmatrix}
 Z_{g,i}\\
 Z_{b,i}
 \end{pmatrix}\stackrel{\text{iid}}{\sim}\mathcal{N}(\bm{\mu}, \Sigma)
 \qquad \bm{\mu}=\begin{pmatrix}
\beta_0+\beta_1\bm{x}_i\\
\alpha_0+\alpha_1\bm{x}_i
 \end{pmatrix}
 \qquad
 \bm{\Sigma}=\begin{pmatrix}
 1&\rho\\
 \rho&1
 \end{pmatrix}
 \label{latentutility}
 \end{equation}
 The premise of this model is that $\rho$ reflects the influence of private information available to the auditor but not the researcher, and $\bm{x}_i$ represents covariates of a company that is available to both the auditor and to the researcher. The observed binary indicators, $G$ and $B$, relate to these latent utilities via
 \begin{align}
 G&=\mathbbm{1}\cbr{Z_{g,i}\geq 0}\\
 B&=\mathbbm{1}\cbr{Z_{b,i}\geq -\gamma G}
 \label{align1}
 \end{align}
 The coefficient $\gamma$ governs the strength of the inducement effect.

The basic identification strategy can be motivated geometrically. Let $$\bm{\Pi}=\begin{pmatrix}
\pi_{01}&\pi_{11}\\
\pi_{00}&\pi_{10}
\end{pmatrix}$$
where $\pi_{jk}=\Pr(B=j, G=k)$, which describes the four scenarios resulting from our equations for $G$ and $B$.  Figure \ref{ellipse} gives a visual representation of the $\bm{\Pi}$ matrix.
\begin{figure}[h]
	\centering
	\begin{subfigure}{.4\textwidth}
	\includegraphics[scale=0.44]{ellipse1}
	\end{subfigure}
	\begin{subfigure}{.4\textwidth}
\includegraphics[scale=0.5]{ellipse2}
	\end{subfigure}
\caption{The bivariate probit entails ellipse shaped probability contours, where (when $\gamma=0$) the probability mass associated to each quadrant represents the four combinations of the bivariate binary observed variables $(B, G)$. The shaded region in the right panel, labeled ``A'', is subtracted from the upper left quadrant and added to the upper right quadrant when a going concern is issued, thus reflecting the endogeneity of the going concern variable. The parameters $\rho$ and $\gamma$ are estimable because changes in the shape of the ellipses, governed by $\rho$, lead to distinct apportioning of probability than do changes in the width of the A region, governed by $\gamma$.}
\label{ellipse}
\end{figure}
Note in Figure \ref{ellipse} that $\bm{\mu}$ determines the location (center of ellipse) and the correlation $\rho$ determines the tilt and concentration of the probability contours. Inducement introduces an extra parameter which lowers the threshold for bankruptcy by $\gamma$.  


\subsection{A causal interpretation of $\gamma$}
Having presumed a particular parametric model for the distribution of the data $(G,B)$ (conditional on covariates $\mathbf{x}$), we would like additional license for the interpretation that $\rho$ captures the contribution of auditor's additional information on bankruptcy likelihood while $\gamma$ captures the contribution of inducement effects on bankruptcy likelihood. To justify this interpretation, we turn to the causal analysis framework of \cite{pearl-2000}. Recall that in Pearl's framework, the inducement effect would be written as 
\begin{equation}\label{docalc}
\mbox{Pr}(B = 1 \mid \mathbf{x}, \text{do}(G = 1)) / \mbox{Pr}(B = 1 \mid \mathbf{x}, \text{do}(G = 0)),
\end{equation}
where $\text{do}(G=1)$ denotes the intervention of issuing a going concern, irrespective of the stochastic data generating process. Denote by $U$ the auditor's additional information.  Suppressing the covariates $\mathbf{x}$, the relationship between $G$ and $B$ can be expressed using the following causal diagram.
\begin{figure}[h!]
\begin{tikzcd}
 & U \arrow[dr] \arrow[dl]\\
G \arrow[rr] && B
\end{tikzcd}
\caption{Conditional on observable attributes $\mathbf{x}$ (not shown), the causal diagram above stipulates the temporal ordering among the firm's private information $U$, the auditor risk assessment $G$, and the firm's bankruptcy outcome, $B$.}\label{causaldiag} 
\end{figure}
This diagram asserts several causal assumptions.  First, the issuance of a going concern does not cause the existence of auditor's additional information:  there is no arrow running from $G$ to $U$.  Second, bankruptcies cannot cause going concerns:  there is no arrow running from $B$ to $G$.  Similarly, bankruptcies do not cause the creation of auditor's additional information for predicting bankruptcy:  there is no arrow from $B$ to $U$.  All of these assumptions follow straightforwardly from a temporal ordering---auditor's first procure information concerning bankruptcy propensity ($U$), they then issue going concern opinions ($G$) and then firms either go bankrupt or not ($B$).  Because $U$ disconnects alternative routes from $B$ to $G$ and no directed path exists from $G$ to $U$, $U$ is said to satisfy the back-door criterion \cite{pearl-2000}, and we can compute $\mbox{Pr}(B = 1 \mid \mathbf{x}, \text{do}(G = 1))$ via the expression:
\begin{equation}\label{backdoor}
\mbox{Pr}(B = 1 \mid \mathbf{x}, \text{do}(G = 1)) =  \int  \mbox{Pr}(B = 1 \mid \mathbf{x}, U=u, G = 1) f(u) \dd u,
\end{equation}
where $f(u)$ is the marginal density of the random variable $U$.

The difficulty, of course, is that $U$ is unobserved in our problem so $f(u)$ can never be estimated from data.  However, we can re-express the bivariate probit model directly in terms of $U$ in order to derive expression (\ref{backdoor}) in terms of parameters $\rho$, $\gamma$, and $\beta$.  This demonstrates how the functional form of the model dictates the causal estimand in (\ref{docalc}), which in turn establishes the causal interpretation of the $\gamma$ parameter.  

In detail, re-writing (\ref{latentutility}) conditional on $U$ gives a model with diagonal error covariance:
\begin{equation}
\begin{split}
\begin{pmatrix}
Z_{g,i} \\ Z_{b,i}
\end{pmatrix} &\sim \mbox{N}(\mu, \Sigma), \;\;\;\; 
\mu  = \begin{pmatrix}
\beta_{0} + \beta_{1}\mathbf{x}_i + \eta_g U  \\
\alpha_{0} + \alpha_{1}\mathbf{x}_{i}  + \eta_b U
\end{pmatrix},  \;\;\;\;  \Sigma = \begin{pmatrix} v_g & 0 \\ 0 & v_b \end{pmatrix},
\end{split}
\end{equation}
where $U \sim \mbox{N}(0, 1)$, $v_g = 1 - \eta_g^2$, $v_b = 1 - \eta_b^2$ and $\rho = \eta_g \eta_b$.  Although this representation is non-unique in $(\eta_g, \eta_b, v_g, v_b)$, it turns out that expression (\ref{backdoor}) will not depend on these values.  This representation allows us to apply the causal assumptions depicted in the causal diagram above, which in turn allows us to derive the counterfactual probability of bankruptcy as:
\begin{equation}
\begin{split}
\mbox{Pr}(B = 1 \mid \mathbf{x}, \text{do}(G = 1)) &=  \int  \mbox{Pr}(B = 1 \mid \mathbf{x}, U=u, G = 1) \N_u(0,1) \dd u,\\
& = \int  1 - \Phi(0; \gamma + \alpha_0 + \alpha_1 \mathbf{x} + \eta_b u) \N_u(0,1) \dd u,\\
& = \int 1 - \int_{-\infty}^0 \N_w(\gamma + \alpha_0 + \alpha_1 \mathbf{x} + \eta_b u, v_b) \dd w \N_u(0,1) \dd u,\\
& = 1 - \int_{-\infty}^0 \int \N_w(\gamma + \alpha_0 + \alpha_1 \mathbf{x} + \eta_b a, v_b)  N_u(0,1) \dd a \dd w,\\
& = 1 - \int_{-\infty}^0 \N_w(\gamma + \alpha_0 + \alpha_1 \mathbf{x}, 1) \dd w,\\
& = 1 - \Phi(0; \gamma + \alpha_0 + \alpha_1 \mathbf{x}),\\ 
&= \Phi(\gamma + \alpha_0 + \alpha_1 \mathbf{x}).
\end{split}
\end{equation}
Here $\Phi(0; \mu)$ denotes the CDF of a normal distribution with mean $\mu$ and variance 1, evaluated at 0.  A similar calculation can be done for $\mbox{Pr}(B = 1 \mid \mathbf{x}, \text{do}(G = 0))$, allowing us to recover the causal risk ratio as $$\tau(\mathbf{x}_i) = \Phi(\gamma + \alpha_0 + \mathbf{x}\alpha_1) /\Phi(\alpha_0 + \mathbf{x}\alpha_1).$$  In other words, fitting a bivariate probit model to the data $(G, B, \mathbf{x})$, coupled with the causal assumptions encoded in the causal diagram (Figure \ref{causaldiag}), implies a causal inducement effect that can be written in terms of $\alpha$ and $\gamma$. Although $\gamma$ is a shared constant parameter, its impact on the risk ratio for a given firm will depend on both $\mathbf{x}$ and $\alpha$. 



\subsection{Identification and estimation for bivariate probit models}\label{techremarks}
The previous section related the parameters of the bivariate probit model with endogenous regressor to the causal risk ratio. However, identifiability is a distinct concern. Identification of parameters in bivariate probit models is subtle and deserves a careful discussion. The treatment in \cite{Heckman-1978} derives the bivariate probit model from a system of simultaneous equations.  Section 3 of \cite{Heckman-1978}, page 949, provides a proof that the associated reduced form parameters of the model are identified without any exclusion restrictions, i.e., the going concern and bankruptcy equations do not share all of their covariates in common.  Identification follows from the functional form of the probit likelihood, and indeed \cite{Heckman-1978} contains a section devoted to maximum likelihood estimation.  \cite{Heckman-1978} also treats the continuous (non-binary response) version of the same structural system; in that case, exclusion restrictions are necessary for identification, and in that case estimation can proceed by a two-stage least squares procedure without specifying a likelihood function.  \cite{Evans-Schwab-1995} study an applied problem using the binary response formulation of the \cite{Heckman-1978} model, but do not assume the probit formulation and rather proceed to estimate parameters using an OLS based procedure.  In this context, the role of an exclusion restriction is an open question as  \cite{Altonji-Elder-Taber-2005} point out; however, the two step procedure applied to the binary response setting gives inconsistent estimates. In summary, textbook treatments of the bivariate probit model equivocate on the necessity of an exclusion restriction \citep[Chapter~15]{Wooldridge-2010}.  To be clear, if one assumes the bivariate probit formulation, then an exclusion restriction is not necessary.  If fitting a generalized linear model to a bivariate binary response {\em without} specifying a link function, it is unknown if (but plausible that) an exclusion restriction is necessary.

Here, these concerns are secondary, as we do not demand identification, but proceed instead via a sensitivity analysis.







\section{Modular sensitivity analysis with machine learning}\label{section_indirect_inference}


In this section we propose our new approach for machine learning-based sensitivity analysis by generalizing the bivariate probit model.  We begin by defining the joint probability of treatment and outcome as
\begin{equation}
\Pr\left(B, G\mid \mathbf{x}\right)=\int_{\mathbb{R}} \Pr\left(B\mid \mathbf{x},U=u,G\right)\Pr\left(G\mid \mathbf{x},U=u\right)f(u)\dd u
\label{eq2}
\end{equation}
for latent variable $U$. In this formulation, $U$ has two special properties. First, it is assumed to be the {\em orthogonal} component of the private information in the sense that $U \independent X$, hence $\mathbf{x}$ does not appear in $f(u)$. Second, $U$ is assumed to be {\em complete}, in the sense that $\Pr(B \mid \mathbf{x}, u, G)$ can be interpreted causally in $G$, because $U$ is a sufficient control variable. That is, $\Pr(B^1 \mid \mathbf{x}, u) = \Pr(B \mid \mathbf{x}, \text{do}(G = 1), u) = \Pr(B \mid \mathbf{x}, G = 1, u)$ and similarly for $G = 0$; accordingly, the inducement effect for firm $i$ is
\begin{equation}
\tau(\mathbf{x}_i)\equiv\frac{\int_{\mathbb{R}}\Pr\left(B=1\mid \mathbf{x},G=1,u\right)f(u)\dd u }{\int_{\mathbb{R}}\Pr\left(B=1\mid \mathbf{x},G=0,u\right)f(u)\dd u}.
\label{realtreat}
\end{equation}
Because the outcome and treatment are both binary, we can expand this probability into its four constituent parts.  For convenience, we specify a probit link, yielding 
\begin{align}
\begin{split}
\Pr\left(B=1\mid \mathbf{x},U=u,G=1\right)& = \Phi\left(b_1(\mathbf{x})+u\right),\\
\Pr\left(B=1\mid \mathbf{x},U=u,G=0\right)&= \Phi\left(b_0(\mathbf{x}\right)+u), \\
\Pr\left(G=1\mid \mathbf{x},U=u\right)&= \Phi\left(g(\mathbf{x})+u\right).
\end{split}
\label{maineq}
\end{align}
Therefore, in terms of $f$, $b_1$, $b_0$ and $g$, the individual inducement effect for firm $i$ is
\begin{align}
\tau(\mathbf{x}_i)=\frac{\int_{\mathbb{R}} \Phi\qty(b_1(\mathbf{x})+u)f(u)\dd u }{ \int_{\mathbb{R}} \Phi\qty(b_0(\mathbf{x})+u) f(u)\dd u}
\label{treateq}
\end{align}
and we denote the sample average inducement effect as $\bar{\tau} = \frac{1}{n}\sum_{i=1}^{n}\tau(\mathbf{x}_i)$. Importantly, the orthogonality and completeness of $U$, as well as the choice of the probit link, are not substantive assumptions, as $U$ is unobserved and $b_1$, $b_0$ and $g$ are nonparametric functions of $\mathbf{x}$. Rather, these assumptions {\em define} $U$ and give the specification of $f(\cdot)$ meaning; the choice of $f$, therefore, {\em is} a substantive assumption (as it is in the bivariate probit model as well).







 This formulation entails that as $u\rightarrow -\infty$, the probability of bankruptcy approaches 0, regardless of whether the treatment is administered or not. As $u\rightarrow \infty$, the probability of bankruptcy approaches 1.  The special case $u=0$ corresponds to no unobserved confounding and the inducement effect can be computed directly from the observed joint probabilities.      	
Finally, because $G$ and $B$ must have a valid joint distribution at each $\mathbf{x}$ value, we have the following system of equations defining our data generating process:
\begin{equation}
    \begin{split}
	\Pr\qty(B=1, G=1\mid \bm{x})&= \int_{\mathbb{R}} \Phi\qty(g(\bm{x})+u)\Phi\qty(b_1(\bm{x})+u)f(u)\dd{u},\\
	\Pr\qty(B=1, G=0\mid \bm{x})&=\int_{\mathbb{R}} \qty(1-\Phi\qty(g(\bm{x})+u))\Phi\qty(b_0(\bm{x})+u)f(u)\dd{u},\\
	\Pr\qty(B=0, G=1\mid \bm{x})&=\int_{\mathbb{R}}\Phi\qty(g(\bm{x})+u)\qty(1-\Phi\qty(b_1(\bm{x})+u))f(u)\dd u.
	\label{long}
\end{split}
\end{equation}
Observe that this generalizes the bivariate probit model with endogenous regressor: when $U \sim \mbox{N}(0, \rho/(1-\rho))$,  $b_0(\mathbf{x}) = \alpha_0 + \alpha_1 \mathbf{x}$, $b_1(\mathbf{x}) = \alpha_0 + \alpha_1 \mathbf{x} + \gamma$, and $g(\mathbf{x}) = \beta_0 + \beta_1 \mathbf{x}$ we recovers that model exactly.	Our formulation is quite a lot more flexible: we relax the Gaussian assumption on the marginal distribution of $U$, drop the parallel relationship between $b_0(\cdot)$ and $b_1(\cdot)$, and allow $b_1$, $b_0$ and $g$ to be nonlinear\footnote{Observe that when the form of $b_1$, $b_0$ and $g$ are constrained, as in the linear probit model, the choice of the probit link becomes a substantive modeling assumption, while in our more flexible formulation it is merely a convenience.}. The price of the extra flexibility of our relaxed specification is that $f(u)$ is now unidentified, whereas in the bivariate probit case it is assumed to be Gaussian but with an identified correlation parameter $\rho$. 
	
The left hand side of the system in (\ref{long}) --- the {\em reduced form} parameters --- can be estimated from the observed data.  Any of a host of machine learning classification methods, such as random forest \citep{rf}, xgboost \citep{boost}, Bayesian additive regression trees (BART) \citep{bart}, among others, can be used to obtain estimates of these probabilities. Here we focus our attention on BART for two reasons: one, we can impose monotonicity so that going concerns can only increase the probability of bankruptcy, and two, we obtain a Bayesian measure of uncertainty based on Markov chain Monte Carlo sampling methods. See Appendix \ref{machine_append} for a brief comparison between BART, gradient boosting, and random forests fit to the bankruptcy data.



\subsection{Projecting the reduced form probabilities onto the causal parameters}
What remains is to solve for $b_1(\cdot),b_0(\cdot), g(\cdot)$, the {\em structural}, or causal, parameters. To do so, we take a numerical approach, by minimizing  the sum of the squared distance between the three left-hand right-hand pairs in (\ref{long}):
\begin{align*}
&\left[\Phi^{-1}\left(\Pr\qty(B=1, G=1\mid \mathbf{x})\right)-\Phi^{-1}\left(\int_{\mathbb{R}} \Phi\qty(g(\mathbf{x})+u)\Phi\qty(b_1(\mathbf{x})+u)f(u)\dd{u}\right)\right]^2+\\
&\left[\Phi^{-1}\left(\Pr\qty(B=1, G=0\mid \mathbf{x})\right)-\Phi^{-1}\left(\int_{\mathbb{R}} \qty(1-\Phi\qty(g(\bm{x})+u))\Phi\qty(b_0(\bm{x}+u))f(u)\dd{u}\right)\right]^2+\\
&\left[\Phi^{-1}\left(\Pr\qty(B=0, G=1\mid \mathbf{x})\right)-\Phi^{-1}\left(\int_{\mathbb{R}}\Phi\qty(g(\bm{x})+u)\qty(1-\Phi\qty(b_1(\bm{x})+u))f(u)\dd u\right)\right]^2.
\end{align*}

Although it is unclear that (\ref{long}) has a unique solution in $b_1$, $b_0$, $g$, numerical solvers converge readily in our experience. Heuristically, as a convex combination of monotone functions, each of the individual integrals in (\ref{long}) is likely to be nearly linear over much of its domain. Note that the use of the normal inverse CDF simply ensures that the range of our objective function is unbounded; we observe that this improves numerical stability of our solver.

We refer to this process as {\em modular} because it requires fitting the reduced form model just one time. Sensitivity of the causal estimates to different choices of $f$ can be assessed independently using the same estimates (or posterior samples) from a single reduced form model fit.




\section{Monotone BART for reduced form inference}
\subsection{Probit BART Overview}
	BART, Bayesian additive regression trees, is at its core a sum-of-trees model. For a $p-$dimensional vector of covariates $\bm{x}$ and a continuous response variable $y$, the BART model is 
	\begin{equation}
	Y=t(\bm{x})+\varepsilon, \qquad \varepsilon\sim \N(0, \sigma^2)
	\label{bart_setup}
	\end{equation}
	where $t(\bm{x})=\E(Y\mid \bm{x})$ denotes a sum of $L$ regression trees, i.e. $t(\bm{x})=\sum_{l=1}^{L} q_l(\bm{x})$. See figure \ref{fig:treestep} for an example regression tree.  In addition to this additive tree representation, BART utilizes a stochastic process tree prior that favors smaller trees; the prior probability of splitting at depth $d$ is $\eta(1+d)^{-\zeta}, \; \eta\in (0,1), \; \zeta\in[0, \infty)$ \citep{Chipman-1998}.    

	At each leaf of the tree parameters are assigned independent regularization priors, $m_{lb}\sim  \N(0, \sigma_{\mu}^2)$, where $\sigma_{\mu}=0.5/(k\sqrt{L})$, where $L$ is the number of trees. 
	
	To handle binary outcomes, BART may be extended through a latent probit formulation, using the data augmentation approach of \cite{Albert-1993}. For binary outcome $B$ 
	\begin{align*}
	&B^*=t(\bm{x})+\varepsilon, \qquad \varepsilon\sim \N(0,1),\\
	&B=\mathbbm{1}(B^*>0),
	\end{align*}
which implies
	\begin{equation}
	\Pr(B=1\mid \bm{x})=\Phi(t(\bm{x}))
	\label{bart_binary}
	\end{equation}
	where $\Phi$ is the standard normal CDF.  

		The $B^*$ variables may be imputed from their truncated normal full conditional distributions; conditional on $B^*$ the BART fitting algorithm can be applied as usual.
	
	
	
		\begin{figure}
		\begin{center}
			\begin{tikzpicture}[
			scale=0.8,
			node/.style={%
				draw,
				rectangle,
			},
			node2/.style={%
				draw,
				circle,
			},
			]
			\node [node] (A) {$x_1<0.8$};
			\path (A) ++(-135:\nodeDist) node [node2] (B) {$m_{l1}$};
			\path (A) ++(-45:\nodeDist) node [node] (C) {$x_2<0.4$};
			\path (C) ++(-135:\nodeDist) node [node2] (D) {$m_{l2}$};
			\path (C) ++(-45:\nodeDist) node [node2] (E) {$m_{l3}$};
			
			\draw (A) -- (B) node [left,pos=0.25] {no}(A);
			\draw (A) -- (C) node [right,pos=0.25] {yes}(A);
			\draw (C) -- (D) node [left,pos=0.25] {no}(A);
			\draw (C) -- (E) node [right,pos=0.25] {yes}(A);
			\end{tikzpicture}
			%
			\hspace{0.1\linewidth}
			\begin{tikzpicture}[scale=3]
			\draw [thick, -] (0,1) -- (0,0) -- (1,0) -- (1,1)--(0,1);
			\draw [thin, -] (0.8, 1) -- (0.8, 0);
			\draw [thin, -] (0.0, 0.4) -- (0.8, 0.4);
			\node at (-0.1,0.4) {0.4};
			\node at (0.8,-0.1) {0.8};
			\node at (0.5,-0.2) {$x_1$};
			\node at (-0.3,0.5) {$x_2$};
			\node at (0.9,0.5) {$m_{l1}$};
			\node at (0.4,0.7) {$m_{l2}$};
			\node at (0.4,0.2) {$m_{l3}$};
			\end{tikzpicture}
		\end{center}
		\caption{(Left) An example binary tree, with internal nodes labelled by their splitting rules and terminal nodes labelled with the corresponding parameters $m_{lb}$. (Right) The corresponding partition of the sample space and the step function.  Figure from \citep{bcf}.}
		\label{fig:treestep}
	\end{figure}
\subsection{Monotone probit BART}
We turn now to a modification of the BART probit model for the bankruptcy and going concern data. We model the left-hand side of the system in (\ref{long}) using a compositional representation, using two ``chained'' regression models, one for $\Pr(G \mid \mathbf{x})$ and another for $\Pr(B \mid \mathbf{x}, G)$. This formulation permits us to insist that $\Pr(B=1\mid G=1, \mathbf{x}) \geq \Pr(B=1\mid G=0, \mathbf{x})$ for all $\mathbf{x}$, encoding the uncontroversial belief that going concern opinions never mitigate bankruptcy risk. To enforce this constraint, we parameterize $\Pr(B=1\mid G, \mathbf{x})$ as follows:
 %
\begin{equation}
    \begin{split}
\Pr(B=1\mid G=1, \mathbf{x}) &= \Phi[h_1(\mathbf{x})],\\
\Pr(B=1\mid G=0, \mathbf{x}) &= \Phi[h_0(\mathbf{x})]\Pr(B=1\mid G=1, \mathbf{x}),
\\ &= \Phi[h_0(\mathbf{x})]\Phi[h_1\mathbf{x})],\\
\Pr(G=1\mid \mathbf{x}) &= \Phi[w(\mathbf{x})].
\end{split}
\end{equation}
For each function $h_0, h_1,$ and $w$ we specify independent BART priors which allows us to fit the treatment and outcome models separately. 

The likelihood for the bankruptcy model is

\begin{equation}
\begin{split}
%\prod_{i=1}^n 
L(h_0, h_1; B, G, \mathbf{X}) =&\prod_{i: G_i=1}\Phi(h_1(\mathbf{x}_i))^{B_i}(1-\Phi(h_1(\mathbf{x}_i)))^{1-B_i} \times \\
&\prod_{i: G_i=0}[\Phi(h_0(\mathbf{x}_i))\Phi(h_1(\mathbf{x}_i))]^{B_i} (1-\Phi(h_0(\mathbf{x}_i))\Phi(h_1(\mathbf{x}_i)))^{1-B_i}\label{eq:bart-da0}.
\end{split}
\end{equation}
This likelihood is challenging: The expression $1- \Phi(h_0(\mathbf{x}_i))\Phi(h_1(\mathbf{x}_i))$ does not factor into separate terms involving the unknown functions $h_0$ and $h_1$, making it difficult to adapt the BART MCMC sampler for posterior inference. To overcome this, we introduce a data-augmented representation that permits updating $h_0$ and $h_1$ independently using standard MCMC for probit BART. 

To begin, note that the first term above (corresponding to $G=1$) involves only $h_1$ so we only need to augment data in the $G=0$ ``arm''. When $G=0$, we relate $B$ to two independent binary latent variables $R_0$ and $R_1$  as follows:
\begin{gather*}
    \Pr(R_0=1\mid\mathbf{x}, G=0) = \Phi(h_0(\mathbf{x})),\\
     \Pr(R_1=1\mid\mathbf{x}, G=0) = \Phi(h_1(\mathbf{x}))\label{eq:Rprobs}
%    B = 1 \text{ if } R_{0} = R_{1} = 1\text{ and } 0 \text{ otherwise.} 
\end{gather*}
and $B = R_0R_1$. Integrating out the latent variables gives $\Pr(B=1\mid\mathbf{x}, G=0) = \Phi(h_0(\mathbf{x}))\Phi(h_1(\mathbf{x}))$ and $\Pr(B=0\mid\mathbf{x}, G=0) = 1-\Phi(h_0(\mathbf{x}))\Phi(h_1(\mathbf{x}))$ as required\footnote{Observe that $\Pr(B=1\mid\mathbf{x}, G=1) = \Pr(R_1=1\mid\mathbf{x}, G=0)$, so thinking about this as a generative model we can interpret $R_1$ as a simulated outcome if we had observed $G=1$ and $R_0$ as an indicator that this outcome is ``thinned'' to enforce monotonicity, since in reality $G=0$.}. 
The augmented likelihood function (including $R_0, R_1$) is 
\begin{equation}
\begin{split}
L(h_0, h_1; R, B, G, \mathbf{X}) =&\prod_{i: G_i=1}\Phi(h_1(\mathbf{x}_i))^{B_i}(1-\Phi(h_1(\mathbf{x}_i)))^{1-B_i} \times \\
&\prod_{i: G_i=0} \Phi(h_1(\mathbf{x}_i))^{R_{1i}}(1-\Phi(h_1(\mathbf{x}_i)))^{1-R_{1i}}\times \\
&\prod_{i: G_i=0}\Phi(h_0(\mathbf{x}_i))^{R_{0i}}(1-\Phi(h_0(\mathbf{x}_i)))^{1-R_{0i}}\times\\
&\prod_{i: G_i=0}\ind{B_i = 1\text{ if }R_{0i}=R_{1i}=1} 
\label{eq:bart-da1}
\end{split}
\end{equation}
After rearranging terms, we have two separate probit likelihoods in $h_0$ and $h_1$ (and the domain restriction in the last term). Conditional on $R_0, R_1$ we can update $h_0, h_1$ using standard probit BART MCMC steps. To update the latent variables $R_{0i}$ and $R_{1i}$, first note that they are fixed at 1 when $B_i=1, G_i=0$. When $B_i=0, G_i=0$, $R_i\equiv (R_{0i}, R_{1i})$ is sampled from:
\begin{equation}
\begin{split}
\Pr(R_i=r\mid h_0, h_1, B_i=0, G_i=0)\propto
&\Phi(h_0(\mathbf{x}_i))^{R_{0i}}(1-\Phi(h_0(\mathbf{x}_i)))^{1-R_{0i}}\times\\
&\Phi(h_1(\mathbf{x}_i))^{R_{1i}}(1-\Phi(h_1(\mathbf{x}_i)))^{1-R_{1i}}\times \\
&\ind{r\neq (1,1)},
\end{split}
\end{equation}
which is the joint probability distribution of the latent variables from Eq.~\eqref{eq:Rprobs}, truncated away from the $R_{0i} = R_{1i}=1$ region\footnote{Formally, this MCMC sampler effects joint updates for $R_i$ and the latent variables in the two probit BART models}.
% To begin, note the identity
% \begin{equation*}
% 1-p_0p_1 = p_0(1-p_1) + (1 - p_0)(1 - p_1) + (1-p_0)p_1
% \end{equation*}
% which holds for any two probabilities $p_0$ and $p_1$; it is derived from the joint probability mass function of two independent Bernoulli random variables, which must sum to one. Next, observe that each of the terms in the sum on the right-hand side are products of factors involving $p_0$ or $p_1$, but not both. This suggests the data-augmented representation of the problematic final term (for the $B_i = G_i = 0$ observations only)
% \begin{equation}
% L(h_0, h_1; G_i, \mathbf{x}_i, R_i) =  \begin{cases} \Phi(h_0(\mathbf{x}_i)) (1 - \Phi(h_1(\mathbf{x}_i))) &\mbox{if } R_i = 1 \\
% (1 - \Phi(h_0(\mathbf{x}_i))) (1 - \Phi(h_1(\mathbf{x}_i)))&\mbox{if } R_i = 2 \\
% (1 - \Phi(h_0(\mathbf{x}_i))) \Phi(h_1(\mathbf{x}_i))&\mbox{if } R_i = 3
%  \end{cases}.
% \label{eq:bart-da}
% \end{equation}
%Critically, the likelihood now factors into terms involving only $h_0$ or $h_1$ (but not both), so the usual Gibbs sampler for probit BART may be applied. In a separate Gibbs step, we impute the ``missing" $R$ for those observations with $G_i=B_i=0$ by a simple application of Bayes rule using the above likelihood, for $h_0$ and $h_1$ fixed.

%and the corresponding $(Z_{i1}, Z_{i2})$ are sampled from their full conditionals, which are truncated normal distributions as in probit BART.




\section{Empirical analysis of bankruptcy data}\label{empirical_section}
In this section we study the question of whether unfavorable going concern opinions cause bankruptcy. We conduct a modular sensitivity analysis based on a monotone BART model fit. This combination allows us to use machine learning methods to learn potentially complex functional forms for the observable data distribution -- while reaping the estimation benefits of imposing monotonicity -- and obtain valid measures of uncertainty for average and subgroup average effects under different assumptions about the distribution of private information.

Data collection is described in \ref{data_section}.  Results are presented in section \ref{sens_analysis}, specifically posterior summaries of firm-year estimated inducement effects as $f(u)$ is varied.  For illustration, several individual firms are investigated in section \ref{individ_firms}.  Finally, firm characteristics which moderate the inducement effect are investigated in section \ref{4.5}.

\subsection{Data}\label{data_section} Data was collected and merged from Audit Analytics, Compustat, and BankruptcyData.com, from a sample period of 2000-2014. The bankruptcy indicator was assigned value of 1 if it occurred within a year of the auditing report.  This was done because auditors are supposed to evaluate risk for periods of time around 1 year, as risk is difficult to determine on longer horizons \citep{paper}. 

The following are the control covariates comprising $\bm{x}$. The data were sourced from  Audit Analytics, Compustat, and BankruptcyData.com, encompassing 2000-2014, with 25,350 firm-year observations.

\small 
\begin{enumerate}\label{covariates}
\itemsep0em 
\item {\tt Log(Assets)}
\item {\tt Leverage}: Ratio of total liabilities to total assets
\item {\tt Investment}: Ratio of short-term investments to total assets
\item {\tt Cash}: Ratio of cash and equivalent to total assets
\item {\tt ROA}: return on assets
\item {\tt Log(Price)}:  Natural log of client's stock price
\item {\tt Intangible assets}: Ratio of intangible assets to total assets
\item {\tt R\&D}: Ratio of research and development expenditure to sales
\item {\tt R\&D missing}: An indicator for missing R\&D expenditure
\item {\tt No S\&P rating}: An indicator for the existence of a S\&P rating
\item {\tt Rating below CCC+}: An indicator for S\&P rating below CCC+
\item {\tt Rating downgrade}: Rating downgrade from above CCC+ to CCC+ or below
\item {\tt Non-audit fees}: The ratio of non-audit fees to total audit fees
\item {\tt  Non-audit fees missing}: An indicator for missing non-audit fees
\item {\tt Years client}: The number of years of a client firm relation
\end{enumerate}
\normalsize
These variables are similar to those used in \cite{paper}, which were inspired by \cite{defond-2002}, and were chosen due to their potential relevance to a companies' upcoming bankruptcy risk as well as their relevance to the issuance of a going concern opinion. 
 
\subsection{Sensitivity to the distribution of private information}\label{sens_analysis}

For fixed conditional probabilities on $(B, G)$ outcomes  (\ref{long}), different choices of $f(u)$ will yield different causal estimates based on solutions to $(b_0, b_1, g)$. Specifically, the right tail of the density $f(u)$ governs how likely an auditor is to observe information that would make a bankruptcy much more likely than suggested by the available covariates, while the left tail governs how likely an auditor is to observe information that would make bankruptcy much less likely than indicated by the available covariates. For reference, in a bivariate probit analysis, $f(u)$ is assumed to have a $\N(0, \sigma)$ distribution, where $\sigma = \sqrt{\rho/(1-\rho)}$; larger $\sigma$ means the available covariates are a more incomplete guide to actual bankruptcy risk. Table \ref{resultssummary_rr}, reports estimated inducement effects for various specifications of $\sigma$, the standard deviation of $U$.


In addition to varying $\sigma$, various asymmetric specifications can be considered, reflecting the belief that the unreported information contained in $U$ is more likely to inflate (or deflate) bankruptcy probabilities. We consider a 3-component Gaussian mixture distributions with varying weights and component means, as well as a skewed unimodal (at zero) model with Gaussian tails called the ``sharkfin'' \citep{hahnslice}, which has the following expression:
\begin{equation}
	\pi(\beta)=\begin{cases}
		2qf(\beta)&\beta\leq 0\\
		2f\qty(\frac{\beta}{1-q}\cdot q)\cdot q&\beta>0
	\end{cases}
	\label{shark}
\end{equation}
where $f(\cdot)$ is the pdf of the normal distribution with standard deviation $s$, and $q$ controls the skew. Table \ref{resultssummary_rr} gives a summary of posterior estimates of the average inducement effect across the firms in our study.  In the first panel of Figure \ref{induceoverrisk}, the sample average inducement effect (causal risk ratio) as a fraction of the observed risk ratio is plotted against $\sigma$ (the stipulated standard deviation of $U$) for various shapes of $f(u)$, demonstrating how distributional assumptions about the auditors' private information affect causal estimates.

\begin{figure}[ht]
	\centering 
	\begin{subfigure}{.5\textwidth}
		\includegraphics[width=5.75cm]{bump_plots}
	\end{subfigure}%
\begin{subfigure}{.5\textwidth}
	\includegraphics[width=5.75cm]{shark_plots}
\end{subfigure}
\caption{The distributions of $f$ affiliated with the results in table \ref{resultssummary_rr}.  On the left are three 3-component Gaussian mixture models, 2 symmetric and one with more weight to the right, encapsulating the belief that events that make bankruptcy more likely are more common than those that ensure no bankruptcy. Right: 3 different normal distributions centered around 0 and two specifications of the sharkfin. }
\label{f_dist_plots}
\end{figure}

\begin{figure}[ht]
	\centering 
	\begin{subfigure}{.5\textwidth}
		\includegraphics[width=6.25cm]{induce_post_bumps}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\includegraphics[width=6.25cm]{induce_post_sharksnorm}
	\end{subfigure}
	\caption{Posterior distributions of inducement effect for distributions from Figure \ref{f_post_plots}. }
	\label{f_post_plots}
\end{figure}
\clearpage
\begin{table}[!httb]
	\centering

	\begin{tabular}{lP{1.9cm}P{2.9cm}}

		\toprule
		Distribution of $f(u)$   &inducement posterior mean &95\% Credible interval for mean inducement \\ \midrule
		E-Value&102.4&$\qty(3.86, 714)$\\
		
		$\N(0,\sigma=0.1)$ &48.7&$\qty(2.14, 337)$\\ %\hline
		$\N(0,\sigma=0.5)$&15.5 &$\qty(1.18,97.4)$\\%\hline      
		$\N(0,\sigma=1)$&2.33&$\qty(1.00, 9.51)$\\
		Shark $q=0.25$, $s=0.5$; $\sigma=1.05$&1.22&$\qty(1.00, 2.48)$\\
		Shark $q=0.75$, $s=1.25$; $\sigma=0.88$&12.6&$\qty(1.05, 80.9)$\\
		
		``Right Bump'' $\sigma=0.48$ &9.73&$\qty(1.00, 76.0)$\\
		98\% peak $\sigma=0.29$&40.1&$\qty(1.53,282)$\\
		90\% peak $\sigma=0.64$&9.30&$\qty(1.00, 72.1)$\\
		%	80\% peak&1.66&4.30&100\\
		
		\bottomrule%\\\midrule %\hline       
		%total&333&435&66&47&& \\ \bottomrule        
	\end{tabular}
	%\captionsetup{labelformat=empty}
	\caption{The reduced form probabilities (\ref{long}) were estimated using BART with a monotonicity constraint on the going concern variable.  We further require $b_1(\bm{x})>b_0(\bm{x})$ in the projection step. Posterior summaries based on 500 Monte Carlo samples. $\sigma$ refers to the implied standard deviations of the different distributions. }
	\label{resultssummary_rr}
	%\end{bclogo}
\end{table}


\begin{figure}[!httb]
	\centering 
		\begin{subfigure}{.5\textwidth}
	\includegraphics[width=7.cm]{induceoverrisk}
		\end{subfigure}%
		\begin{subfigure}{.5\textwidth}
			\includegraphics[width=7.cm]{diff_shark_plots}
		\end{subfigure}
	\caption{Left: Plot of inducement effect over observed risk ratio for different standard deviations $\sigma$ for $f$ normally distributed (red), sharkfin ($q=0.9$) with right skew (black), and sharkfin ($q=0.1$) with left skew (blue). The mean observed risk ratio was 30.80. On right is a plot of what the shark fin with $q=0.1$ and $q=0.9$ and $\sigma=0.50$, for visual purposes. }%the mean naive treatment effect was 9.26\%
	\label{f_explain_plots}
\end{figure}







\subsection{Comparison with the E-value}\label{E-value-comparison}
Rather than modeling the distribution of unobserved information $f(u)$, an alternative approach is to consider the strength of unobserved confounding that would be necessary to entirely explain the observed association. This approach can be found as early as \cite{Cornfield}, and has recently been generalized in \cite{evalue} and \cite{Peng-2016}, who prove that 

\begin{equation}
	\max(\text{RR}_{GU}, \text{RR}_{UB})\geq \cbr{\text{RR}^{\text{obs}}_{GB}+\sqrt{\text{RR}^{\text{obs}}_{GB}\qty(\text{RR}^{\text{obs}}_{GB}-\text{RR}^{\text{true}}_{GB})}}\bigg{/}\text{RR}^{\text{true}}_{GB}
	\label{high_thresh}
\end{equation}
where
\begin{align*}
	\text{RR}_{GU\mid \bm{x}} &= \max_{k} \;\;  \frac{\Pr\qty(U=k\mid G=1, \bm{x})}{\Pr\qty(U=k\mid G=0, \bm{x})},\\
	\text{RR}_{UB\mid \bm{x}} &= \max_{k,k',g} \;\; \frac{\Pr\qty(B=1\mid G=g, \bm{x}, U=k)}{\Pr\qty(B=1\mid G=g, \bm{x}, U=k')} 
\end{align*}
for $g \in \lbrace 0,1\rbrace$ and
\begin{equation}
	\text{RR}^{\text{true}}_{GB}=\frac{\int \Pr(B=1\mid G=1, \bm{x}, U)\Pr(U\mid \bm{x})\dd u}{\int \Pr(B=1\mid G=0, \bm{x}, U)\Pr(U\mid \bm{x})\dd u}.
	\label{true_causal_rr}
\end{equation} 
Figure \ref{rel_risk_table} provides a visualization of these terms.

	\begin{figure}[ht]
	\centering
	\begin{minipage}{.6\textwidth}

	\begin{tikzpicture}[thick,scale=.65, every node/.style={scale=.65}]
		\node (X) at (0,0) {$\boxed{\text{Going concern}}$};
		\node at (0,.5) {$G$};
		\node (Y) at (-4,0)  {Observed Covariates};
		\node at (-4,-.5) {$\bm{x}$};
		\node (Z) at (3,2) {Unobserved};
		\node at (3,2.5){$U$};
		\node (U) at (6,0)  {$\boxed{\text{Bankruptcy}}$};
		\node at (6,.5) {$B$};
		\draw[thick, -Latex](X)--(U);%node [midway,below,sloped] ;
		%	\node at (3,0) [above] {Causality?};
		\draw [thick,-Latex] (Y) -- (X);
		%	\draw [thick,dashed, <->] (Y) -- (Z);
		\draw [thick, dashed, >=triangle 45, <->] (Y) to[bend right=-30] (Z);
		\draw [thick,-Latex] (Y) to[bend right=30] (U);
		\draw [thick,-Latex] (Z) -- (X) node [midway,below,sloped] {$\text{RR}_{GU}$};
		%	\draw [thick,-Latex] (Y) -- (U) ;
		\draw[thick,-Latex] (Z) -- (U)  node [midway,below,sloped] {$\text{RR}_{UB}$};
		%\draw [semithick] (Z) -- (U) node [midway,below,sloped] {$\RR_{UD}$};
	\end{tikzpicture}

\end{minipage}%
%\begin{minipage}[t][-.5cm][b]{.5\textwidth}
%	\scalebox{.8}{
%		\begin{tabular}{@{}lccc@{}} 
%			\toprule 
%			&  & \multicolumn{2}{c}{Going Concern Opinion}\\\cmidrule{3-4} 
%			& Bankrupt & Yes & No\\ \midrule 
%			& Yes & $\Pr(B=1,G=1)$& $\Pr(B=1,G=0)$\\
%			& No & $\Pr(B=0,G=1)$& $\Pr(B=0, G=0)$  \\ \bottomrule 
%		\end{tabular}
%	}
%\end{minipage}
	\caption{$\text{RR}_{GU}$ is the maximum risk ratio comparing any two categories of confounding and $\text{RR}_{UB}$ is the
	maximum risk ratio for any specific level of the unmeasured confounders comparing those with and without treatment, controlling for $\bm{x}$.}
\label{rel_risk_table}
\end{figure}


Setting $\text{RR}_{GB}^{\text{true}}=1$ in expression \ref{high_thresh}, \cite{Peng-2016} define the {\em E-value} (for evidence value) as
\begin{equation}
	\text{E-value} = \text{RR}^{\text{obs}}_{GB}+\sqrt{\text{RR}^{\text{obs}}_{GB}\qty(\text{RR}^{\text{obs}}_{GB}-1)},
	\label{eval_full}
\end{equation} 
which can be interpreted as the minimum strength of association that an unmeasured confounder would need to have with both the $G$ and $B$ (conditional on $\bm{x}$) to fully explain the observed treatment-outcome association. Note that for large observed risk ratios (that is, $RR^{\text{obs}} \approx RR^{\text{obs}} - 1$), the E-value is essentially proportional to the observed risk ratio itself. Accordingly, if we compare our model-based sensitivity analysis estimates to the E-value, we find that when $f(u)$ concentrates around zero, the associated causal risk ratio becomes the observed risk ratio, which is effectively the E-value. However, for different choices of $f(u)$, the associated causal risk ratio at different $\bm{x}$ values can differ from the observed risk ratio in interesting ways, which we explore in the following sections. Figure \ref{E-val-ratio_audit} plots posterior means of $\tau$ against the posterior mean of the E-value for the auditing data for the distributions of $U$ reported in \ref{E-val-ratio}. Essentially, E-values are simply a scale multiple of the observed risk ratio, which is precisely the causal risk ratio when there is assumed to be no private information (lower right panel of Figure \ref{E-val-ratio_audit}). However, less dogmatic choices of $f(u)$ also yield substantial inducement effect estimates for some firms (first three panels of Figure \ref{E-val-ratio_audit}).
\begin{figure}[h]
	\centering

	\includegraphics[height=8.25cm]{Eval_vs_U_ratio_constrained_png}

	\caption[E-val vs ratio]{  Posterior means of $\tau$ across 500 draws for different distributions of $f(u)$ vs the E-value per firm calculated from the posterior mean of the risk ratio from $\text{RR}^{\text{obs}}_{GB\mid \bm{x}}=\Pr\qty(B=1\mid G=1, \bm{x})/\Pr\qty(B=1\mid G=0, \bm{x})$. }
	\label{E-val-ratio_audit}
	
\end{figure}

\subsection{Posterior Individual Inducement Effects for Specific Firms}\label{individ_firms}
By numerically solving \ref{long} for $(b_0, b_1, g)$ at each posterior draw, for a given firm-year observation and a given choice of $f(u)$, a full posterior distribution over causal estimands for that observation can be obtain.  Scrutinizing these posteriors for specific firms provides an intuitive approach to investigating the results of the sensitivitiy analysis that is more granular than simply reporting sample averages across all observations. To this end, the posterior mean inducement effect, as well as a 95\% credible interval, are presented in table \ref{individ_firm_table} for a selection of illustrative firms.  Figure \ref{individ_firm_plot} depicts a histogram of the posterior inducement effect estimates for Apple (from year 2001) and Jetblue (from 2007) using $f\sim$ ``right-bump'' in our integration step.
  
\begin{table}[h]
	\centering

	\begin{subtable}{.71\textwidth}
		\scalebox{0.65}{
			\begin{tabular}{lP{1.1cm}P{1.1cm}P{1.1cm}P{1.1cm}P{1.1cm}P{1.cm}P{1.cm}P{1.8cm}}
				\toprule
				Firm   &Going Concern&Bankruptcy& Auditor&mean $\text{RR}_{\text{obs}}$&mean $B_0$& mean $B_1$&mean $\tau$ post &95\% Credible interval for $\tau$ (\%)  \\ \midrule
				Jetblue (2007)&No&No&E\&Y&45.0&0.005&0.062&16.8&$(2.32, 50.0)$\\
				%\rowcolor{navy!49!white}
				Jetblue (2009)&No&No&E\&Y&10.9&0.015&0.050&3.94&$(1.00, 12.6)$\\
				%\rowcolor{navy!49!white}
				Apple (2001) &No&No&KPMG&403&0.001&0.062&103.7&$(6.62, 461)$\\
				Build a Bear (2010)&No&No&KPMG&23.3&0.004&0.021&7.23&$(1.13, 24.3)$\\
				Build a Bear (2014)&No&No&E\&Y&49.1&0.004&0.039&15.1&$(2.62, 55.7)$\\
				Radioshack (2013)&No&No&PWC&4.66&0.122&0.297&2.99&$(1.02, 7.74)$\\
				Radioshack (2014)&No&Yes&PWC&3.03&0.143&0.251&1.98&$(1.00, 4.64)$\\
				Blockbuster (2004)&No&No&PWC&42.9&0.003&0.024&13.6&$(1.48, 54.8)$\\
				Blockbuster (2009)&Yes&No&PWC&4.89&0.082&0.222&3.02&$(1.15, 6.25)$\\
				Six Flags (2006)&No&No&KPMG&14.4&0.017&0.084&6.13&$(1.36, 15.3)$\\
				Six Flags (2009)&Yes&Yes&KPMG&3.78&0.136&0.307&2.49&$(1.03, 5.66)$\\
				\bottomrule       
			\end{tabular}
		}
	\end{subtable}%
\vline\vline
	\begin{subtable}{.71\textwidth}
		\scalebox{.65}{
			\begin{tabular}{P{1.cm}P{1.cm}P{1.cm}P{1.8cm}}
	
				\toprule
				mean $B_0$& mean $B_1$&mean $\tau$ post &95\% Credible interval for $\tau$ (\%)  \\ \midrule
				0.008&0.045&7.45&$(1.07, 25.3)$\\
				%\rowcolor{navy!49!white}
				0.022&0.038&2.32&$(1.05, 7.52)$\\
				%\rowcolor{navy!49!white}
				0.003&0.042 &43.6&$(1.02, 408)$\\
			0.007&0.027&5.31&$(1.08, 11.1)$\\
				0.006&0.048&9.49&$(1.08, 25.3)$\\
				0.132&0.262&2.26&$(1.00, 5.79)$\\
				0.151&0.231&1.63&$(1.00, 3.99)$\\
				0.007&0.016&4.10&$(1.07, 11.2)$\\
		0.093&0.156&1.75&$(1.00. 4.13)$\\
				0.021&0.032&1.78&$(1.00, 4.96)$\\
			0.144&0.307&2.32&$(1.00, 5.089)$\\
				\bottomrule       
			\end{tabular}
		}
	\end{subtable}
	%	\captionsetup{labelformat=empty}
	\caption[Different ACRR estimates for specific firms]{Left:Posterior estimates of the inducement  effect given $f(u)\sim \N(0, \sigma=0.5)$, here we look at the mean of the posterior estimates of the inducement effect for different firms in our dataset.  Right: The distribution of $f(u)$ is an asymmetric 3-component Gaussian mixture model, the ``right-bump'', see figure \ref{individ_firm_plot},  with more weight on large $u$, encoding the belief that events that make bankruptcy more likely are more common than those that ensure no bankruptcy. }
	\label{individ_firm_table}
	%	\end{bclogo}
\end{table}
\begin{figure}[h]
	\centering	\begin{minipage}{.3\textwidth}
		\includegraphics[width=4cm]{rightbump2}
	\end{minipage}%
	\begin{minipage}{.5\textwidth}
		
		\includegraphics[width=8.5cm]{Apple_jetblue_right_RR}
	\end{minipage}
	\caption[Apple vs Jetblue]{Histogram of posterior estimates of the individual inducement effects given $f(u)$ with the distribution on the left (moderate confounding) for Apple 2001 and Jetblue 2007.  Neither received a going concern, nor did either go bankrupt.  Jetblue was audited by Ernst and Young, and Apple audited by KPMG.} 
	\label{individ_firm_plot}
\end{figure}
We find that the inducement effect varies both across posterior draws as well as across firms as a function of the density $f(u)$.  Differences between firms are illuminating: For example, Apple in 2001 had a significantly higher inducement effect than Blockbuster in 2009, but this is at least in part an artifact of Apple 2001 having extremely low bankruptcy probability.

\subsection{Exploratory subgroup analysis}\label{4.5} 
With firm-year specific treatment effects in hand, one can conduct an ex post regression tree analysis to isolate subgroups of firms with subgroup average treatment effects that depart from the overall average.
Specifically, we identify moderating subgroup of variables by fitting a single regression tree using the individual inducement effect estimates (posterior means) as the response variable and observable firm (and auditor) features as predictors (as detailed in \cite{bcffreak}).   For predictors we use the same covariates reported in \ref{covariates}, all of which are plausible moderators of the inducement effect.  

The subgroup analysis presented here is based on $U \sim \N(0, \sigma=0.5)$ to the left hand side of equation(\ref{long}). The left panel of Figure \ref{cart_tree_RR} shows the resulting tree fit. Using this tree, we can identify subgroups based on the corresponding partition implied by terminal node (leaf) membership. However, the resulting point estimates only tell part of the story; for a fuller picture, we can consider the posterior distribution of subgroup {\em differences}, even for different choices of $f(u)$ than the one used to produce the tree.  We compute the subgroup difference of mean inducement effects for each posterior draw between the subgroups with the largest and smallest subgroup effects as determined by the regression tree.  This analysis is repeated for 4 different distributions of $f(u)$: $f_1(u)\sim \N(0,\sigma=.5)$, $f_2\sim \N(0,1)$, $f_3(u)$  is a a mixture model with more weight on a far bump to the right, see figure \ref{individ_firm_plot}, and $f_4(u)$ is a 3 component Gaussian mixture with 90\% of the area centered around 0, and 5\% around $u=-2$ and $u=2$ respectively. The right panel of Figure \ref{cart_tree_RR} shows posteriors of subgroup differences in inducement effects (causal risk ratios); the sign of the differences is preserved across various choice, while the magnitude varies (as one might anticipate).

At this point it is instructive to consider if different estimands may be moderated by different covariates. In particular, risk ratios may be dominated by the denominator, which may be affected by different variables than those which affect the numerator. Accordingly, we also fit a regression tree to point estimates of the (causal) risk difference $\Pr(B =1\mid \bm{x}, \text{do}(G=1))-\Pr(B=1\mid \bm{x}, \text{do}(G=0))$. While risk ratios can be unappealingly large for firms with very small bankruptcy risk, risk differences (necessarily) have the opposite complication, which is that a difference of 0.1 ``means'' something quite different for a firm with control probability of 0.5 than it does for one with control probability 0.9. Fortunately, the risk difference has another interpretation in contexts like the present one where treatment effects are assumed to be monotonic: the risk difference is equivalent to the probability that a firm went bankrupt {\em because of} the going concern opinion. This interpretation is derived as follows. Consider the four possible potential outcomes, depicted in table \ref{tab:induce_explain_table}, which gives each configuration a suggestive name.
\begin{table}[h]
	\centering
	\begin{tabular}{lP{1cm}P{1.5cm}}

		\toprule
		Name&$B^{1}$&$B^{0}$\\ \midrule
		No Inducement&0&0\\
		Prevention&0&1\\
		Induced bankruptcy&1&0\\
		No prevention&1&1\\
		
		\bottomrule%\\\midrule %\hline       
		%total&333&435&66&47&& \\ \bottomrule        
	\end{tabular}
	%\captionsetup{labelformat=empty}
	\caption{Because we are operating in the binary treatment/binary response world, we have just 4 outcomes.  The first row refers to firms who irregardless of receiving going concern opinion avoid bankruptcy, hence the name ``no harm''.  Prevention refers to situation where without the treatment, the firm would've gone bankrupt, but with the going concern opinion did not, a situation we do not allow for with our monotonicity assumption $\Pr(B=1\mid \bm{x}, G=1)\geq \Pr(B=1\mid \bm{x}, G=0)$. Non-prevention means regardless of going concern opinion being issued, company would go bankrupt. Induced bankruptcy refers to the firm going bankrupt because of the going concern opinion.  }
	\label{tab:induce_explain_table}
	%\end{bclogo}
\end{table}
The marginal probabilities are then simply the sum of rows where 1 appears in the corresponding column of Table \ref{tab:induce_explain_table}:
\begin{equation}
\begin{split}
	\Pr(B=1\mid \bm{x}, \text{do}(G=0))&=\Pr(\text{prevention})+\Pr(\text{no prevention})\\
\Pr(B=1\mid \bm{x}, \text{do}(G=1))&=\Pr(\text{induced bankruptcy})+\Pr(\text{no prevention})
\end{split}
\end{equation}
But, under the monotonicity assumption, $\Pr(\text{prevention}) = 0$, in which case
\begin{equation}
	\Pr(B=1\mid \bm{x}, \text{do}(G=1))-\Pr(B=1\mid \bm{x}, \text{do}(G=0))=\Pr(\text{induced  bankruptcy}).
	\label{prevent_eq}
\end{equation}

%Our findings in this section show there is considerable moderation for certain subgroups, though they tend to be be comprised of a small number of observations compared to the total number of firm years in our dataset (25,350).  Unsurprisingly, the posterior estimates of the inducement effect for subgroups shift toward 1 as the effect of $U$ grows larger. This is evidenced on the right panels of figures \ref{cart_tree_RR} and \ref{cart_tree_treat}.

\begin{figure}[!httb]
		\centering
	\begin{subfigure}{.5\textwidth}


	\includegraphics[ width=7cm]{cart_tree_RR_smallertree.pdf}



\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
%	\centering
	\includegraphics[ width=7cm]{RR_post_diff}


\end{subfigure}
	\caption{Left: A small tree fit to inducement effects (risk ratios).  This is also the group of variables we investigate as moderators. Follow down tree to identify subgroup.  Right: Plot of difference of inducement effects across the posterior draws between the largest and smallest inducement effect subgroups (bottom right and bottom left respectively on the tree). }
		\label{cart_tree_RR}
		%\label{mod_plot_RR}
\end{figure}
\clearpage
\begin{figure}[!httb]
	\centering
	\begin{subfigure}{.5\textwidth}
	
	\includegraphics[width=7cm]{cart_tree_treatment.pdf}
		\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
			\includegraphics[ width=7cm]{treat_post_diff}
		\end{subfigure}
	\caption{Left: A small tree fit to the risk difference, $\Pr(B =1\mid \bm{x}, \text{do}(G=1))-\Pr(B=1\mid \bm{x}, \text{do}(G=0))$, which is under monotonicity of going concerns is equivalent to the probability that the bankruptcy was induced.  Right: Posterior subgroup differences between the largest and smallest treatment effect subgroups (bottom right and bottom left respectively on the tree).}
	\label{cart_tree_treat}
\end{figure}
\begin{table}[h]
\begin{subtable}{.6\textwidth}

	\begin{tabular}{llP{1.4cm}P{1.4cm}}
	
		\toprule
		Subgroup&Mean Inducement&Mean $B_1$&Mean $B_0$\\ \midrule
	1 & 3.87 & 0.022 & 0.011 \\ 
	2 & 17.51 & 0.038 & 0.007 \\ 
	3 & 6.31 & 0.082 & 0.027 \\ 
	4 & 11.41 & 0.084 & 0.021 \\ 
	5 & 4.83 & 0.156 & 0.055 \\ 
		\bottomrule
	\end{tabular}
\end{subtable}%
\begin{subtable}{.6\textwidth}
\scalebox{.89}{
	\begin{tabular}{llll}
		
		\toprule
Subgroup&	Mean Treatment &Mean $B_1$&Mean $B_0$\\ \midrule
1 &	 0.033&0.056&0.023\\
2 &	0.039&0.043&0.004\\
3 &	0.022&0.030&0.008\\
4 &	 0.094&0.125&0.032\\
5 &	 0.044&0.050&0.006 \\ 
6 &		0.037&0.039&0.002\\
		\bottomrule
	\end{tabular}
}
\end{subtable}
	%\captionsetup{labelformat=empty}
		\caption{Left: Compare with treatment tree \ref{cart_tree_treat}, bottom left to right of subgroups. Right: Compare with inducement tree \ref{cart_tree_RR}, bottom left to right of subgroups. $B_1$ refers to $\Pr(B =1\mid \bm{x}, \text{do}(G=1))$ and $B_0$ refers to $\Pr(B=1\mid \bm{x}, \text{do}(G=0))$. }
		\label{tree_sub_table}
%	\label{tree_sub_table_RR}

\end{table}

\begin{figure}[!httb]
		\centering
	\begin{subfigure}{.5\textwidth}
		
		\includegraphics[width=7cm]{cart_tree_B1.pdf}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\includegraphics[ width=7.5cm]{B1_post_diff.pdf}
	\end{subfigure}
	\caption{Left: A small tree fit to the $\Pr(B=1\mid \bm{x}, \text{do}(G=1))$, $B_1$ for shorthand.  Right: Plot of differences of $B_1$  across the posterior draws between the largest and smallest $B_1$ effect subgroups (bottom right and bottom left respectively on the tree).}
	\label{B1_tree_fig}
\end{figure}
\begin{figure}[!httb]
	\centering
	\begin{subfigure}{.5\textwidth}
		
		\includegraphics[width=7cm]{cart_tree_B0.pdf}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\includegraphics[ width=7.5cm]{B0_post_diff.pdf}
	\end{subfigure}
	\caption{Left: A small tree fit to the $\Pr(B=1\mid \bm{x}, \text{do}(G=0))$, $B_0$ for short.  Right: Plot of differences of $B_0$  across the posterior draws between the largest and smallest $B_0$ effect subgroups (bottom right and bottom left respectively on the tree).}
	\label{B0_tree_fig}
\end{figure}
\color{black}


Likewise, the same procedure can be conducted for $\Pr(B=1\mid \bm{x}, \text{do}(G=0))$ itself. {\color{red} The differences between the variables appearing in the three summary trees above is interesting.}


\section{Simulation studies}\label{sim_study}
In this section we present how our method performs under a variety of different simulated settings. We begin by comparing our method to the bivariate probit regression model in \ref{bivar_simsec}, and show that we perform comparably well when the data is generated according to the bivariate probit, and in \ref{nonlindgp} we show how better than the bivariate probit under more complicated non-linear data generating processes. \ref{nonlindgp} also explores how mis-specification of $f$ affects our modeling.  Specifically, we explore the effects of mis-specifying the scale, location, skew, and tail weights of different distributions and report on those effects.  Section \ref{E-val_sec} repeats the results of figure \ref{E-val-ratio_audit} but with simulated data.  In \ref{mono_section}, we show the benefits of using the monotonicity constraint in BART.
\subsection{Comparison to bivariate probit}\label{bivar_simsec} To verify that the proposed machine learning sensitivity analysis yields sensible answers, we take advantage of the relationship between our model and the bivariate probit model with endogenous binary regressor: if we generate the data from the bivariate probit model with $U \sim \mbox{N}(0, \sigma=\sqrt{\rho/(1-\rho)})$, the true causal risk ratios should be recoverable\footnote{Note, the success of our sensitivity analysis is predicated upon  minimizing the squared distance between the three left hand-side pairs is equation(\ref{long}). We use Nelder-Mead to do so, a commonly used numerical method for minimization of loss functions \citep{nelder} (although we also employed a simulated annealing approach and the Broydenâ€“Fletcherâ€“Goldfarbâ€“Shanno algorithm, both giving similar results as Nelder-Mead)}.  Table \ref{bivartable} reports the results of fitting our sensitivity analysis model data generated from the bivariate probit
\[
\begin{pmatrix}
	Z_{g,i}\\
	Z_{b,i}
\end{pmatrix}\stackrel{\text{iid}}{\sim}\mathcal{N}(\bm{\mu}, \bm{\Sigma})
\qquad \bm{\mu}=\begin{pmatrix}
	\beta_0+\beta_1\bm{x}_i\\
	\alpha_0+\alpha_1\bm{x}_i
\end{pmatrix}
\qquad
\bm{\Sigma}=\begin{pmatrix}
	1&\rho\\
	\rho&1
\end{pmatrix}
\label{model3}
\]
We simulated 25,000 samples, where we sum 5 uniform(-1,1) $\bm{x}_i$ covariates each with the same $\beta_1$ and $\alpha_1$ coefficients respectively.  We set $\beta_0=0, \beta_1=-0.2, \alpha_0=-0.5, \alpha_1=-0.5$ to generate reasonable number of going concerns and bankruptcies.  We fit the left hand side of equation(\ref{long}) using BART with the monotonicity constraint, whose benefit is shown in section[\ref{mono_section}].  Note, in our simulations, we do not solve our systems for every BART posterior estimate of equation \ref{long} due to computational constraints.  Instead, we take the mean of the posterior BART probability estimates in the fitting stage and then solving for our causal parameters $b_0(\vdot), b_1(\vdot), g(\vdot)$ once for each observation\footnote{This methodology held for all the simulated data; when analyzing the real data we repeated the integrals for random samples of the posterior BART estimates}.  We impose the constraint that $b_1(\bm{x})\geq b_0(\bm{x})$ when solving for the causal parameters.


 In the supplemental file, we show that at $\N=100,000$ the bivariate probit regression (unsurprisingly) works remarkably well when the data generation process is the bivariate probit\footnote{It is well-known that maximum likelihood estimates of the bivariate probit model can be unstable (i.e., many local modes), especially when there are a large number of predictor variables (see \cite{Meng-Schmidt-1985} and \cite{Freedman-Sekhon-2010}). Our simulations bear this out; with thousands of observations, estimates of $\rho$ were quite inaccurate. Therefore, to verify that we obtain consistent parameter estimates with maximum likelihood estimation (and to cross-check our data generating process) we  generate and train our models on 100,000 observations.}. Table \ref{bivartable} shows that our method works well even with $\N=25,000$ and $p=5$.  The bivariate probit regression should perform better when correctly specified, but the broadly similar estimates in this case is reassuring that the projection approach is trustworthy.
 \begin{table}[ht]
 	\centering
 	%	\scalebox{.9}{
 	\begin{tabular}{lllllP{1.3cm}}
 		\toprule
 		$\gamma$&$\rho$&	ACRR true&ACRR est& ICRR cor&ICRR rmse \\ 
 		\midrule
 		1.00&0.25&  2.90 & 2.94 & 0.88 & 1.12 \\ 
 		1.75&0.25&  5.35 & 5.08 & 0.88 & 3.74 \\ 
 		2.50&0.25&  8.34 & 7.37 & 0.89 & 11.03 \\ 
 		1.00&0.40 & 2.90 & 2.82 & 0.86 & 1.06 \\ 
 		1.75&0.40&  5.35 & 4.99 & 0.90 & 3.57 \\ 
 		2.50&0.40&  8.34 & 6.74 & 0.89 & 12.83 \\ 
 		1.00&0.60&   2.90 & 2.77 & 0.83 & 1.18 \\ 
 		1.75&0.60&  5.35 & 4.68 & 0.86 & 4.45 \\ 
 		2.50&0.60&  8.34 & 6.75 & 0.85 & 13.53 \\ 
 		1.00&0.80&  2.90 & 2.23 & 0.61 & 1.82 \\ 
 		1.75&0.80& 5.35 & 3.35 & 0.67 & 6.97 \\ 
 		2.50&0.80 &8.34 & 4.53 & 0.67 & 18.99 \\
 		\bottomrule
 	\end{tabular}
 	%}
 	
 	\caption{We simulate from the bivariate probit with 25,000 observations and deploy our methodology.  cor refers to the correlation between predicted and true for the individual causal risk ratios, and the rmse is the root mean square error.}
 	\label{bivartable}
 \end{table}
 


\subsection{Sensitivity to $f$}\label{nonlindgp}



We do much better with our methodology when the data were generated from a non-linear data generating process, as described below:
\begin{align}
	\begin{split}
		b_0(\bm{x})&=\bm{x}_5+\bm{x}_1 \sin(2\bm{x}_6)-1.75\\
		b_1(\bm{x})&=b_0(\bm{x})+1.5\\
		g(\bm{x})&=0.5 b_0(\bm{x})+\bm{x}_2+0.25\\
		U&\sim \mbox{N}(\mu,\sigma^2)\\
		G&\sim \text{Bin}\qty(\Phi(g(\bm{x})+u)) \\
		B\mid G=1&\sim\text{Bin}\qty(\Phi(b_1(\bm{x})+u))\\
		B\mid G=0&\sim \text{Bin}\qty(\Phi(b_0(\bm{x})+u))
	\end{split}
	\label{newnonlinmodel}
\end{align}
where we draw $u$ and $b_i(\cdot)$, $G$ conditional on those values, and subsequently  the values of $B$ are drawn conditional on our values of $G$. The $\bm{x}_i$ are drawn uniform(-1,1), with some $\bm{x}_i$ passed as covariates in our monotone BART fitting stage that do not appear in the DGP; these extraneous variables serve as ``noise'' to complicate the problem and make it more realistic.  Table \ref{nonlintable} demonstrates how in this setting our model performs much better than the bivariate probit.
%\subsection{Mis-specifying Distribution of $U$}\label{mis_spec_append}
Additionally, we mis-specify $f(u)$ to see if we can still return true ITE's, and if we fail, what type of distributions cause problems.  In table(\ref{nonlintable}), we mis-specify with Laplacian distributions, as the fatter tail weight could be problematic, and the table confirms this does appear to be an issue.  Additionally, we compare our methodology with the bivariate probit model, fit with regression spline smoothing and without.  Our methodology does comparatively much better in this setting, as the DGP is highly non-linear. 

In table(\ref{nonlintablesharkfin}), we generate $f(u)$ according to the shark fin but with $\sigma$ varied to attain certain variances.  The choice of $q$ affects the skewness of the distribution.  The shark fin provides us insight into whether or not skewness or large variances affect our models estimates; as the previous table showed mean offsets do not seem to impact our estimates too badly.   In table \ref{nonlintablesharkfin_2}, we see getting $q$ wrong (skewness) seems less impactful, meanwhile downwardly estimating variance seems to bias the estimates of the ACRR up, while guessing variance too high downwardly biases the ACRR.  Table \ref{RRT_RRC_compare} investigates more drastically mis-specifying $q$ or $\sigma$.

\begin{table}[h]
	\scalebox{0.9}{
		\begin{tabular}{lP{1.2cm}P{1.2cm}P{.7cm}lP{1.cm}P{.8cm}P{.8cm}P{.8cm}P{.8cm}P{.8cm}}
			%{P{1.14cm}P{.71cm}P{.8cm}P{1.cm}P{2.05cm}P{1.05cm}P{.8cm}P{.8cm}P{.8cm}P{.8cm}P{.8cm}}
			\toprule
			$f(u)$& \textbf{true} ACRR&  true est.   ACRR&  RMSE  & Wrong $f(u)$  & Wrong est. &  Wrong RMSE&LBP est.& LBP RMSE&SBP est.&SBP RMSE \\ 
			\midrule
			$\N(0,1)$ &  4.43 & 4.71  &1.66 &Lap(0,$1.2$ )&2.02&3.09&4.19&1.98&4.46&2.00\\
			$\N(0,1.5)$  & 2.80 & 2.81 &0.70 &Lap(0, $1.75)$&1.68&1.36&3.22&0.85&3.38&0.94\\
			$\N(0, 2)$  & 2.14& 2.11 &  0.36& Lap(0, $2.5$)&1.42&0.83&0.44&1.81&0.37&0.73 \\
			$\N(0,2.5 )$  & 1.81 & 1.80 & 0.25&Lap(0, $2$)&2.04&0.37&1.81 &0.46&0.37&1.46\\ 
			%	$\N-1, 9)$  & 0.15 & 0.08 &0.61&&&\\ 
			$\N-1, 1)$ & 8.18 & 9.38 & 5.07&Lap(-1, $1.3$)&1.53&7.83&2.96&6.51&2.83&6.61\\ 
			$\N1, 2)$  & 1.74 & 1.45 & 0.34&Lap(1, $2.4$)&1.20&0.57&1.89&0.30&0.62&1.15\\ 
			%	$\N1, 1)$  & 0.29 & 0.27 & 0.39&& &\\ 
			$\N-2, 2)$  & 3.43 & 5.88 & 4.32&Lap($-2, 2.3$)&1.02&2.49&2.77&0.92&3.03&0.76\\
			$\N2,1)$  & 1.68 & 1.62&  0.23&Lap($2, 1.3$)&1.18&1.39&0.61&0.45&1.75&1.42\\ \bottomrule
		\end{tabular}
	}
	\caption{Different $f(u)$ as described in DGP(\ref{newnonlinmodel}).  $\N=25,000$.  Wrong $f(u)$ indicates the distribution of $U$ we used to solve the system of equations in \ref{maineq}, i.e. how we mis-specified.  True indicates true average causal risk ratio, and correct est. indicates our estimate of the ACRR when \emph{correctly} specifying $f(u)$. We use standard deviation instead of variance for our spread parameter. Lap refers to the Laplacian distribution. LBP refers to bivariate probit regression without smoothing, and SBP refers to bivariate probit regression with smoothing covariates, i.e.  where the smooth term for each covariate is made of basis functions.}
	\label{nonlintable}
\end{table}

\begin{table}[h]
	\centering
	\scalebox{.9}{
		\begin{tabular}
			{P{2.2cm}P{.6cm}P{1.6cm}P{.8cm}lP{.9cm}P{1.9cm}}
			%{P{2.cm}P{.6cm}P{.8cm}P{.8cm}P{1.7cm}P{.9cm}P{.9cm}P{1.7cm}P{.9cm}P{.9cm}}
			\toprule
			
			$f(u)$
			sharkfin with parameters $q$, $s$& \textbf{true} ACRR&  true est. ACRR& true  RMSE  & wrong $q$   & wrong $q$ est. &  wrong $q$  RMSE  \\ 
			\midrule
			(0.25, 0.82; 3) &  1.79 & 1.81 &0.21 &(0.40,1.37;3.00)&1.80&0.23\\
			(0.40, 1.37; 3)  & 2.07 & 2.08 &0.31 &(0.70,2.34;3.00)&2.55&0.94 \\
			(0.60, 1.06; 3)  & 3.10 & 2.97 &  0.80& (0.30,1.00;3.00)&1.97&1.43 \\
			(0.75, 2.46; 3)  & 5.86 & 5.37 & 2.59&(0.92,2.77;3.00)&8.41&5.13\\ 
			(0.25, 0.34; 0.5) & 4.11 & 4.27 & 1.54&(0.10,0.12;0.50)&4.13&1.44\\ 
			(0.40, 0.56; 0.5)  & 5.28 & 5.95 & 2.71&(0.20,0.26;0.50)&5.25&2.01\\ 
			(0.60, 0.84; 0.5) & 8.63 & 8.80 &5.31&(0.80,1.05;0.50)&10.9&7.60\\ 
			(0.75, 1.00; 0.5)  & 13.4 & 12.3 & 8.25&(0.45,1.63;0.50)&7.74&10.6\\ \bottomrule
		\end{tabular}
	}
	\caption[Mis-specifying distribution: check skewness]{Different $f(u)$ as described in DGP(\ref{newnonlinmodel}), all of the ``sharkfin'' family.  $\N=25,000$. Wrong q indicates that we purposely mis-specified q when solving our system of equations, whereas the true est.columns indicate where we correctly specified $f(u)$, both the $q$ and $s$ parameters, when solving our system. ; indicates the variance, whereas the first 2 entries in shark are the $q$ and $s$ parameters. Here we vary the skewness while keeping variance constant.}
	\label{nonlintablesharkfin}
\end{table}
\begin{table}[h]
	\centering
	\scalebox{.9}{
		\begin{tabular}
			{P{2.2cm}P{1.cm}P{1.6cm}P{.8cm}lP{.9cm}P{.9cm}}
			%{P{2.cm}P{.6cm}P{.8cm}P{.8cm}P{1.7cm}P{.9cm}P{.9cm}P{1.7cm}P{.9cm}P{.9cm}}
			\toprule$f(u)$
			sharkfin with parameters $q$, $s$& \textbf{true} ACRR& true  est. ACRR & true  RMSE   & wrong $\sigma^2$  & wrong $\sigma^2$ est. &  wrong $\sigma^2$  RMSE\\ 
			\midrule
			(0.25, 0.82; 3) &  1.79 & 1.76  &0.38 &(0.25,0.47;1.0)&3.55&2.12\\
			(0.40, 1.37; 3)  & 2.07 & 2.09 &0.61 &(0.40,1.12;2.0)&2.76&0.90 \\
			(0.60, 1.06; 3)  & 3.10 & 3.27 &  1.54& (0.60,0.92;0.6)&11.3&10.4 \\
			(0.75, 2.46; 3)  & 5.86 & 7.40 & 7.94&(0.75,1.74;1.5)&9.79&6.20\\ 
			(0.25, 0.34; 0.5) & 4.11 & 5.34 & 6.25&(0.25,0.67;2.0)&1.56&3.16\\ 
			(0.40, 0.56; 0.5)  & 5.28 & 8.91 & 10.7&(0.40,1.12;2.0)&1.83&4.28\\ 
			(0.60, 0.84; 0.5) & 8.63 & 9.65 & 22.5&(0.60,2.38;4.0)&1.40&9.29\\ 
			(0.75, 1.00; 0.5)  & 13.4 & 16.6 & 57.2&(0.75,3.18;5.0)&1.90&15.5\\ \bottomrule
		\end{tabular}
	}
	\caption[Mis-specifying distribution: check variance change]{Different $f(u)$ as described in DGP(\ref{newnonlinmodel}), all of the ``sharkfin'' family.  $\N=25,000$. Wrong $\sigma^2$ indicates that we purposely mis-specified our variance (by varying the $\sigma$ parameter) when solving our system of equations, whereas the true est. columns indicate where we correctly specified $f(u)$, both the $q$ and $a$ parameters, when solving our system. ; indicates the variance, whereas the first 2 entries in shark are the $q$ and $s$ parameters. Here we vary the variance keeping skewness constant.  }
	\label{nonlintablesharkfin_2}
\end{table}

\begin{table}[ht]
	\centering
	\scalebox{0.9}{
		\begin{tabular}{lP{1.cm}P{1.cm}P{1.cm}P{1.cm}lP{1.6cm}P{1.6cm}}
			%|c|>{\raggedright}p{6.5cm}|c|
			%{P{3.3cm}P{1.cm}P{1.cm}P{1.cm}P{3.3cm}P{1.cm}P{1.cm}}
			\toprule
			
			True $f(u)$ & true ACRRT   &ACRRT est.&ACRRC true &ACRRC est.& Wrong q $f(u)$&ACRRT est. wrong& ACRRC est. wrong\\ 
			\midrule 
			shark(0.1, 0.30; 3) & 1.60 &1.62&1.67&1.69 &shark(0.9, 2.74;3)&1.36&1.60 \\ 
			shark(0.1, 0.12; 0.5) & 3.18&0.40&3.79& 3.75&shark(0.9, 1.12; 0.5)&3.82&5.19 \\
			shark(0.1, 0.18; 1)&2.32&2.39&2.58&2.69&shark(0.9, 1.58; 1)&2.75&3.72\\
			shark(0.1, 0.18; 1)&2.32&2.39&2.58&2.69&shark(0.5, 1; 1)&2.43&2.97\\
			shark(0.5, 1; 1)&4.00&4.18&4.66&5.18&shark(0.1, 0.18; 1)&3.16&3.47\\
			shark(0.5, 1; 1)&4.00&4.18&4.66&5.18&shark(0.9, 1.58; 1)&6.78&9.32\\
			shark(0.9, 1.58; 1)&13.1&12.0&19.2&17.6&shark(0.1, 0.18; 1)&2.75&2.56\\
			shark(0.9, 1.58; 1)&13.1&12.0&19.2&17.6&shark(0.5, 1; 1)&4.96&5.66\\
			\bottomrule
		\end{tabular}
	}
	\caption[Mis-specifying distribution: check skewness for bigger q, ACRRT and ACRRC]{Comparing estimates of average causal risk ratio on treated (ACRRT) and average causal risk ratio on controls (ACRRC) when we more aggressively mis-specify the q parameter, which controls the skewness. }
	\label{RRT_RRC_compare}
\end{table} 



\subsection{Relationship with E-values: Simulations} \label{E-val_sec}


	Here, we replicate the analysis presented in figure \ref{E-val-ratio_audit} with simulated data.  Rather than using all the posterior draws given by the BART model in the simulated data setting, we instead take the mean of the posterior BART probability estimates in the fitting stage and then solving for our causal parameters $b_0(\vdot), b_1(\vdot), g(\vdot)$ once for each observation.  We impose the constraint that $b_1(\bm{x})\geq b_0(\bm{x})$ when solving for the causal parameters. We do this for different distributions of $f$ with the data generated in accordance with \ref{newnonlinmodel}.   In figure \ref{E-val-ratio}, we compare our estimate of the inducement effect vs the E-value, for different distributions of $U$.  
\begin{figure}[h]
	\centering

	\includegraphics[height=8.25cm]{eval_vs_our_sim}

	\caption[E-val vs ratio]{  Comparison of our individual causal risk ratio estimates vs individual E-value estimates for 25,000 simulations drawn from the same dgp as specified in \ref{newnonlinmodel}.  Shown are different distributions of $f(u)$, with the bottom right ``low u'' setting mimicking the E-value as expected. }
	\label{E-val-ratio}
	
\end{figure}

\subsection{Value of monotonicity}\label{mono_section}
The monotonicity constraint proved valuable, as figure \ref{monovsnorm} shows the improvement we see in estimating the individual causal risk ratios (ICRR) by using monotone BART instead of BART.  The reason for this is that even though BART estimates each potential outcome well, it does not estimate the coupling of the two potential outcomes\footnote{For more on issues with fitting BART estimates in causal inference settings, see \cite{bcf}.}.  Monotone BART helps remedy this by not allowing for going concerns to lower the probability of bankruptcy.  


\begin{figure}[h]
	\centering
	\begin{subfigure}{.5\textwidth}
		\includegraphics[width=7.cm]{monobart_ICRR_png.png}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\includegraphics[width=7.cm]{Bart_ICRR_png.png}
	\end{subfigure}
	\caption[Comparing Bart with and without monotinicity constraint]{Plots of expected individual causal risk ratios vs our estimates, i.e. a plot comparing the ratio of potential outcomes from model \ref{latentutility} ($\Phi(\alpha_0+\alpha_1\bm{x}_i+\gamma)/\Phi(\alpha_0+\alpha_1\bm{x}_i)$) versus our estimate within the integral of equation(\ref{treateq}).  In the DGP, $\rho=0.25,\gamma=1$.  The monotone BART correlation between the truth and estimate is 0.88 and BART is 0.83.}
	\label{monovsnorm}
\end{figure}













\section{Discussion}
%Our primary interest in this paper is to compare our flexible sensitivity analysis to the E-value analysis, by expanding upon the more traditional, but less expressive bivariate probit with endogenous regressor. Our model is similar to the E-value in that we can quantify how much confounding we would need to explain away a causal effect.  However, our model gives a fully Bayesian posterior interpretation in addition to coming equipped with a powerful non-parametric tool to control for covariates.  Additionally, our sensitivity analysis allows the distribution of confounding to be explicitly chosen, which allows for additional interpretation over the single number given by the E-value in determining how much confounding is necessary to explain away the causal effect observed.  

In comparison to the bivariate probit model, the machine learning sensitivity analysis approach is more flexible, albeit at the expense of identification.  The bivariate probit model guarantees identification, but the cost is stricter assumptions and the corresponding reduction in empirical validity. % One major benefit of the probit model, however, is that the computation needed is much less than the sensitivity analysis, as there is no system of equations to solve.  However, the probit requires a maximum likelihood estimate, and the smoothing of covariates also adds more computational strain.  
%While this data set consisted of about ~25,000 observations, were it to continuously be curated over time and expand year over year, it could soon become intractable to perform the sensitivity analysis barring access to a cluster or a workstation computer with significant parallelization available.  However, as our system of equations is easily parallelizable.  

The high level summary of our empirical findings is that at least some firms appear to experience induced bankruptcies; the degree of private information would have to be extreme to rule this out entirely. Moreover, it appears that {\color{red} THESE VARIABLES make inducement more likely}.

%With regards to the audit problem at hand, we were able to show that there is some inducement effect resulting from auditors' issuing going concerns and subsequent probability of bankruptcy for firms for whom the concern was issued.  The degree to which the inducement effect plays a role is determined by the belief of what the true distribution of $U$ is.  In this regard, the sensitivity analysis presented plays a crucial role, as multiple distributions of $U$ are presented, each giving different results with regard to the estimate of the inducement effect.  


Simulation studies, referenced in the supplement and table \ref{nonlintable}, show our method recovers the true estimand across a variety of different configurations modeling the latent information $U$ (albeit with biased estimates in cases of $U$ with large variance) so long as $f(u)$ is not grossly misspecified.  %While we would like to show the integrals on the right hand side of equation (\ref{long}) have a unique solution, and ideally find an analytic solution, these simulations indicate that our optimization and integration techniques are working well.

Ongoing work include expanding this methodology to a wide range of empirical problems.  In particular, problems which mirror the ``self-fulfilling prophecy'' of the audit problem are of interest,  as the treatment is influenced by the latent information $U$ in a way that admits plausible modeling. For example, the question whether catholic high schools lead to higher college enrollment \citep{Evans-Schwab-1995} would be of particular interest, as that analysis employed the bivariate probit with endogenous regressor approach.


 

%Finally, for the sake of validation, we would like to perform a thorough instrumental variable analysis.  That is, incorporate more data and search for candidate instruments and then compare our results to those from this instrumental variable analysis.









\section*{Acknowledgements}
The authors would like to acknowledge support from NSF grant \#1502640 .

\clearpage
\appendix





































% Acknowledgements should go at the end, before appendices and references



% Manual newpage inserted to improve layout of sample file - not
% needed in general before appendices/bibliography.

\clearpage 
\bibliography{sample}

\end{document}

